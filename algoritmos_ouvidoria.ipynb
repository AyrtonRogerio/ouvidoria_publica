{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#instalacao\n",
        "!pip install fasttext\n",
        "!pip install accelerate -U\n",
        "\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "_U6dSvlS7aff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwaXlYTKNYT4"
      },
      "outputs": [],
      "source": [
        "# Célula 1: Instalações e Configuração do Ambiente\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datetime import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 2: Configuração, Carga e Análise Inicial\n",
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Projeto_Ouvidoria/Data/ouvidoria_sintetico.csv'\n",
        "\n",
        "DATASET_ID = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path, sep=';',encoding='utf-8', on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O arquivo não foi encontrado no caminho '{file_path}'.\")\n",
        "    print(\"Verifique se o caminho no Google Drive está correto.\")\n",
        "    df = None\n",
        "\n",
        "\n",
        "\n",
        "if df is not None:\n",
        "    df_encoded = df.copy()\n",
        "    for column in df_encoded.columns:\n",
        "        if df_encoded[column].dtype == 'object':\n",
        "            df_encoded[column] = LabelEncoder().fit_transform(df_encoded[column])\n",
        "\n",
        "    correlation_matrix = df_encoded.corr()\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(f'Matriz de Correlação - {DATASET_ID}', fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ANUica1BOPrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 3: Pré-processamento e Estratégia de Validação\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load('pt_core_news_sm', disable=['parser', 'ner'])\n",
        "except OSError:\n",
        "    os.system('python -m spacy download pt_core_news_sm')\n",
        "    nlp = spacy.load('pt_core_news_sm', disable=['parser', 'ner'])\n",
        "\n",
        "try:\n",
        "    stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "def preprocess_text_advanced(text):\n",
        "    if not isinstance(text, str) or not nlp: return \"\"\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    doc = nlp(text.lower())\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stopwords_pt and len(token.lemma_) > 2]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "\n",
        "df_model = df.copy()\n",
        "df_model.dropna(subset=['Texto da Demanda', 'Categoria'], inplace=True)\n",
        "print(df_model.shape)\n",
        "print(\"\\nIniciando pré-processamento avançado do texto (lematização).\")\n",
        "df_model['texto_processado'] = df_model['Texto da Demanda'].apply(preprocess_text_advanced)\n",
        "print(\"Pré-processamento concluído.\")\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_model['categoria_encoded'] = le.fit_transform(df_model['Categoria'])\n",
        "\n",
        "\n",
        "X = df_model['texto_processado']\n",
        "y = df_model['categoria_encoded']\n",
        "print(f\"Dataset final com {len(df_model)} amostras preparado para o treinamento.\")\n",
        "\n",
        "\n",
        "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
        "N_RUNS = rskf.get_n_splits(X, y)\n",
        "print(f\"Estratégia de validação definida: {N_RUNS} execuções por modelo.\")\n"
      ],
      "metadata": {
        "id": "GV7cV-NOOWxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 4: Avaliação dos Modelos Scikit-learn\n",
        "\n",
        "\n",
        "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "execution_folder_name = f\"Execucao_{DATASET_ID}_{current_time}\"\n",
        "output_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/Projeto_Ouvidoria/Results/', execution_folder_name)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Resultados desta execução serão salvos em: {output_dir}\")\n",
        "\n",
        "models_to_run = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1500, C=10),\n",
        "    \"Multinomial Naive Bayes\": MultinomialNB(alpha=0.1),\n",
        "    \"Linear SVM\": LinearSVC(dual='auto', C=10)\n",
        "}\n",
        "\n",
        "for name, model in models_to_run.items():\n",
        "    print(f\"\\nAvaliando o modelo: {name}\")\n",
        "    fold_results = []\n",
        "    num_classes = len(le.classes_)\n",
        "    sum_cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "    for i, (train_index, val_index) in enumerate(rskf.split(X, y), 1):\n",
        "        print(f\"   Executando Fold {i}/{N_RUNS}...\", end='\\r')\n",
        "        # ... (código de treino e predição) ...\n",
        "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
        "        X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
        "        X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
        "\n",
        "        model.fit(X_train_fold_tfidf, y_train_fold)\n",
        "        y_pred = model.predict(X_val_fold_tfidf)\n",
        "\n",
        "        labels_ordered = le.transform(le.classes_)\n",
        "        cm = confusion_matrix(y_val_fold, y_pred, labels=labels_ordered)\n",
        "        sum_cm += cm\n",
        "\n",
        "        # ... (cálculo de métricas)\n",
        "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
        "        precision = precision_score(y_val_fold, y_pred, average='macro', zero_division=0)\n",
        "        recall = recall_score(y_val_fold, y_pred, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_val_fold, y_pred, average='macro', zero_division=0)\n",
        "        FP = cm.sum(axis=0) - np.diag(cm)\n",
        "        FN = cm.sum(axis=1) - np.diag(cm)\n",
        "        TP = np.diag(cm)\n",
        "        TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "        fold_results.append({\n",
        "            'Run': i, 'Accuracy': accuracy, 'F1_Score_Macro': f1,\n",
        "            'Precision_Macro': precision, 'Recall_Macro': recall,\n",
        "            'TP': TP.sum(), 'FP': FP.sum(), 'FN': FN.sum(), 'TN': TN.sum()\n",
        "        })\n",
        "\n",
        "\n",
        "    detailed_results_df = pd.DataFrame(fold_results)\n",
        "    base_filename = f\"{DATASET_ID}_detailed_results_{name.replace(' ', '_')}.csv\"\n",
        "\n",
        "    full_path_to_save = os.path.join(output_dir, base_filename)\n",
        "    detailed_results_df.to_csv(full_path_to_save, index=False, float_format='%.5f')\n",
        "    print(f\"\\nResultados detalhados para '{name}' salvos.\")\n",
        "\n",
        "\n",
        "    avg_cm = sum_cm #/ N_RUNS\n",
        "    cm_df = pd.DataFrame(avg_cm, index=le.classes_, columns=le.classes_)\n",
        "\n",
        "    cm_filename = f\"{DATASET_ID}_avg_cm_{name.replace(' ', '_')}.csv\"\n",
        "\n",
        "    cm_full_path = os.path.join(output_dir, cm_filename)\n",
        "    cm_df.to_csv(cm_full_path, float_format='%.2f')\n",
        "    print(f\"Matriz de confusão média para '{name}' salva.\")\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.title(f'Matriz de Confusão Média ({N_RUNS} execuções) - {name}')\n",
        "    plt.ylabel('Classe Verdadeira')\n",
        "    plt.xlabel('Classe Prevista')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nAvaliação dos modelos TF-IDF concluída!\")\n",
        "print(\"=====================================================\\n\")"
      ],
      "metadata": {
        "id": "xpbskV-pOZon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 5: Modelo FastText\n",
        "\n",
        "import fasttext\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "fold_results_ft = []\n",
        "num_classes = len(le.classes_)\n",
        "sum_cm_ft = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(rskf.split(X, y), 1):\n",
        "    print(f\"Executando Fold {i}/{N_RUNS}...\", end='\\r')\n",
        "    train_file = 'fasttext.train'\n",
        "    with open(train_file, 'w', encoding='utf-8') as f:\n",
        "        for idx in train_index:\n",
        "            label = \"__label__\" + str(df_model['Categoria'].iloc[idx]).replace(' ', '_')\n",
        "            text = X.iloc[idx]\n",
        "            f.write(f\"{label} {text}\\n\")\n",
        "\n",
        "    model_ft = fasttext.train_supervised(input=train_file, epoch=25, lr=1.0, wordNgrams=2, dim=320)\n",
        "\n",
        "    X_val_fold_list = X.iloc[val_index].tolist()\n",
        "    y_val_fold_encoded = y.iloc[val_index]\n",
        "\n",
        "    preds_raw = model_ft.predict(X_val_fold_list, k=1)\n",
        "    y_pred_text = [pred[0].replace('__label__', '').replace('_', ' ') for pred in preds_raw[0]]\n",
        "    y_pred_encoded = le.transform(y_pred_text)\n",
        "\n",
        "\n",
        "    labels_ordered = le.transform(le.classes_)\n",
        "    cm = confusion_matrix(y_val_fold_encoded, y_pred_encoded, labels=labels_ordered)\n",
        "    sum_cm_ft += cm\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_val_fold_encoded, y_pred_encoded)\n",
        "    precision = precision_score(y_val_fold_encoded, y_pred_encoded, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_val_fold_encoded, y_pred_encoded, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_val_fold_encoded, y_pred_encoded, average='macro', zero_division=0)\n",
        "    FP = cm.sum(axis=0) - np.diag(cm)\n",
        "    FN = cm.sum(axis=1) - np.diag(cm)\n",
        "    TP = np.diag(cm)\n",
        "    TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "    fold_results_ft.append({\n",
        "        'Run': i, 'Accuracy': accuracy, 'F1_Score_Macro': f1,\n",
        "        'Precision_Macro': precision, 'Recall_Macro': recall,\n",
        "        'TP': TP.sum(), 'FP': FP.sum(), 'FN': FN.sum(), 'TN': TN.sum()\n",
        "    })\n",
        "\n",
        "\n",
        "detailed_results_df_ft = pd.DataFrame(fold_results_ft)\n",
        "base_filename_ft = f\"{DATASET_ID}_detailed_results_FastText.csv\"\n",
        "\n",
        "full_path_to_save_ft = os.path.join(output_dir, base_filename_ft)\n",
        "detailed_results_df_ft.to_csv(full_path_to_save_ft, index=False, float_format='%.5f')\n",
        "\n",
        "avg_cm_ft = sum_cm_ft #/ N_RUNS\n",
        "cm_df_ft = pd.DataFrame(avg_cm_ft, index=le.classes_, columns=le.classes_)\n",
        "\n",
        "cm_filename_ft = f\"{DATASET_ID}_avg_cm_FastText.csv\"\n",
        "\n",
        "cm_full_path_ft = os.path.join(output_dir, cm_filename_ft)\n",
        "cm_df_ft.to_csv(cm_full_path_ft, float_format='%.2f')\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_df_ft, annot=True, fmt='.2f', cmap='Greens')\n",
        "plt.title(f'Matriz de Confusão Média ({N_RUNS} execuções) - FastText')\n",
        "plt.ylabel('Classe Verdadeira')\n",
        "plt.xlabel('Classe Prevista')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "41p0m1v0OeRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 6: BERTimbau\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fold_results_bert = []\n",
        "num_classes = len(le.classes_)\n",
        "sum_cm_bert = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(rskf.split(X, y), 1):\n",
        "\n",
        "    train_df_fold = pd.DataFrame({'text': X.iloc[train_index], 'label': y.iloc[train_index]})\n",
        "    val_df_fold = pd.DataFrame({'text': X.iloc[val_index], 'label': y.iloc[val_index]})\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df_fold).map(tokenize, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = Dataset.from_pandas(val_df_fold).map(tokenize, batched=True, load_from_cache_file=False)\n",
        "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    model_bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./bert_results_fold_{i}',\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=4.246e-05,\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_ratio=0.054,\n",
        "        weight_decay=0.023,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        logging_strategy=\"no\",\n",
        "        save_strategy=\"no\",\n",
        "        disable_tqdm=False,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=model_bert, args=training_args, train_dataset=train_dataset)\n",
        "    trainer.train()\n",
        "\n",
        "    predictions_output = trainer.predict(val_dataset)\n",
        "    y_true_encoded = predictions_output.label_ids\n",
        "    preds_encoded = np.argmax(predictions_output.predictions, axis=1)\n",
        "\n",
        "    labels_ordered = le.transform(le.classes_)\n",
        "    cm = confusion_matrix(y_true_encoded, preds_encoded, labels=labels_ordered)\n",
        "    sum_cm_bert += cm\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_true_encoded, preds_encoded)\n",
        "    precision = precision_score(y_true_encoded, preds_encoded, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true_encoded, preds_encoded, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true_encoded, preds_encoded, average='macro', zero_division=0)\n",
        "    FP = cm.sum(axis=0) - np.diag(cm)\n",
        "    FN = cm.sum(axis=1) - np.diag(cm)\n",
        "    TP = np.diag(cm)\n",
        "    TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "    fold_results_bert.append({\n",
        "        'Run': i, 'Accuracy': accuracy, 'F1_Score_Macro': f1,\n",
        "        'Precision_Macro': precision, 'Recall_Macro': recall,\n",
        "        'TP': TP.sum(), 'FP': FP.sum(), 'FN': FN.sum(), 'TN': TN.sum()\n",
        "    })\n",
        "\n",
        "\n",
        "detailed_results_df_bert = pd.DataFrame(fold_results_bert)\n",
        "base_filename_bert = f\"{DATASET_ID}_detailed_results_BERTimbau.csv\"\n",
        "\n",
        "full_path_to_save_bert = os.path.join(output_dir, base_filename_bert)\n",
        "detailed_results_df_bert.to_csv(full_path_to_save_bert, index=False, float_format='%.5f')\n",
        "\n",
        "avg_cm_bert = sum_cm_bert #/ N_RUNS\n",
        "cm_df_bert = pd.DataFrame(avg_cm_bert, index=le.classes_, columns=le.classes_)\n",
        "\n",
        "cm_filename_bert = f\"{DATASET_ID}_avg_cm_BERTimbau.csv\"\n",
        "\n",
        "cm_full_path_bert = os.path.join(output_dir, cm_filename_bert)\n",
        "cm_df_bert.to_csv(cm_full_path_bert, float_format='%.2f')\n",
        "print(f\"Matriz de confusão média para 'BERTimbau' salva.\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_df_bert, annot=True, fmt='.2f', cmap='OrRd')\n",
        "plt.title(f'Matriz de Confusão Média ({N_RUNS} execuções) - BERTimbau')\n",
        "plt.ylabel('Classe Verdadeira')\n",
        "plt.xlabel('Classe Prevista')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8Y08f8jFOirs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 7: Relatório Final, Comparativo e Visual - CORRIGIDA NOVAMENTE\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "results_parent_dir = '/content/drive/MyDrive/Colab Notebooks/Projeto_Ouvidoria/Results/'\n",
        "results_dir = None\n",
        "\n",
        "try:\n",
        "\n",
        "    all_run_folders = [d for d in os.listdir(results_parent_dir) if os.path.isdir(os.path.join(results_parent_dir, d))]\n",
        "    if not all_run_folders:\n",
        "        raise FileNotFoundError(\"Nenhuma pasta de execução foi encontrada no diretório 'Results'.\")\n",
        "\n",
        "\n",
        "    latest_run_dir = max([os.path.join(results_parent_dir, d) for d in all_run_folders], key=os.path.getmtime)\n",
        "    results_dir = latest_run_dir\n",
        "    print(f\"Usando a pasta de execução mais recente: {results_dir}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Erro: {e}\")\n",
        "\n",
        "if results_dir:\n",
        "\n",
        "    files_found = [f for f in os.listdir(results_dir) if f.endswith('.csv') and 'detailed_results' in f]\n",
        "\n",
        "\n",
        "    if files_found:\n",
        "        try:\n",
        "            sample_df = pd.read_csv(os.path.join(results_dir, files_found[0]))\n",
        "            print(f\"\\nColunas encontradas no arquivo de exemplo '{files_found[0]}':\")\n",
        "            print(sample_df.columns.tolist())\n",
        "        except Exception as e:\n",
        "            print(f\"Não foi possível ler o arquivo de exemplo para verificar as colunas: {e}\")\n",
        "\n",
        "    final_results_list = []\n",
        "    print(f\"\\nGerando Relatório Final...\\n\")\n",
        "\n",
        "    for file_name in files_found:\n",
        "        try:\n",
        "            model_name = file_name.split('_detailed_results_')[-1].replace('.csv', '').replace('_', ' ')\n",
        "            full_path = os.path.join(results_dir, file_name)\n",
        "            df_result = pd.read_csv(full_path)\n",
        "\n",
        "            final_results_list.append({\n",
        "                'Modelo': model_name,\n",
        "                'Acurácia Média': df_result['Accuracy'].mean(),\n",
        "                'Acurácia Desv. Padrão': df_result['Accuracy'].std(),\n",
        "                'F1-Score Médio': df_result['F1_Score_Macro'].mean(),\n",
        "                'F1-Score Desv. Padrão': df_result['F1_Score_Macro'].std(),\n",
        "                'Precisão Média': df_result['Precision_Macro'].mean(),\n",
        "                'Precisão Desv. Padrão': df_result['Precision_Macro'].std(),\n",
        "                'Recall Média': df_result['Recall_Macro'].mean(),\n",
        "                'Recall Desv. Padrão': df_result['Recall_Macro'].std(),\n",
        "            })\n",
        "            print(f\"-> Resultados de '{model_name}' processados com sucesso.\")\n",
        "        except KeyError as e:\n",
        "            print(f\"-> ERRO DE CHAVE (Coluna não encontrada): {e}. Verifique se o nome da coluna está correto no código e no CSV para o arquivo '{file_name}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"-> ERRO ao processar o arquivo '{file_name}': {e}\")\n",
        "\n",
        "    if not final_results_list:\n",
        "        print(\"\\nNenhum arquivo de resultado foi processado. A análise não pode continuar.\")\n",
        "    else:\n",
        "\n",
        "        final_summary_df = pd.DataFrame(final_results_list)\n",
        "        final_summary_df_display = final_summary_df.sort_values(by='F1-Score Médio', ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "        display(final_summary_df_display)\n",
        "\n",
        "\n",
        "        summary_base_filename = 'resumo_geral_modelos.csv'\n",
        "        summary_full_path = os.path.join(results_dir, summary_base_filename)\n",
        "        final_summary_df_display.to_csv(summary_full_path, index=False, float_format='%.5f')\n",
        "\n",
        "\n",
        "\n",
        "        def plot_metric_comparison(dataframe, metric_col, std_col, title_suffix):\n",
        "            plot_df = dataframe.sort_values(by=metric_col, ascending=True)\n",
        "            fig, ax = plt.subplots(figsize=(12, 7))\n",
        "            bars = ax.barh(plot_df['Modelo'], plot_df[metric_col], xerr=plot_df[std_col], align='center', alpha=0.85, ecolor='black', capsize=5)\n",
        "            ax.bar_label(bars, fmt='%.4f', padding=5, fontsize=10)\n",
        "            ax.set_xlabel(metric_col.replace('_', ' '))\n",
        "            ax.set_ylabel('Modelo')\n",
        "            ax.set_title(f\"Comparação de Modelos {title_suffix}\")\n",
        "            if not plot_df.empty:\n",
        "                ax.set_xlim(left=max(0, plot_df[metric_col].min() * 0.95), right=plot_df[metric_col].max() * 1.05)\n",
        "            ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        plot_metric_comparison(final_summary_df, 'F1-Score Médio', 'F1-Score Desv. Padrão', 'por F1-Score')\n",
        "        plot_metric_comparison(final_summary_df, 'Acurácia Média', 'Acurácia Desv. Padrão', 'por Acurácia')"
      ],
      "metadata": {
        "id": "GP5kEIa4Olua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}